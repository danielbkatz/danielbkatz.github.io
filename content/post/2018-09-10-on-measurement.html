---
title: "On Correlation"
author: "Danny"
date: '2018-09-10'
slug: on-measurement
tags: StatsED
categories:
- Measurement
- Correlation
- Causality
- StatsED
---



<p>There’s a problem with the various versions of the oft-heard-in-intro-stats-courses phrase “correlation is not causation.” It gives early statistics students the wrong impressions.</p>
<p>The idea seems simple: Just becuase there is statistical correlation in the data present does not mean that that there is a real world causal relationship (a complicated term, it turns out) between x and y. You know, x could have a high correlation with y but actually, w is simply a mutual cause of x and y. Or something like that. It protects from students finding a high correlation and going “ah ha, it turns out swimming causes sun burns” (I guess it could, water rinses off sunscreen, and you suddenly have a sunburn)</p>
<p>The phrase is problematic because it’s actually not accurate.</p>
<p>1.) The first problem is that, actually, when in the realm of statistics, the only thing that may indicate a causal relationship is correlation. The more accurate phrase might be something like, “correlation does not <strong><em>necessarily</em></strong> imply causation”</p>
<p>2.) The next problem is that, paradoxically, the phrase does not go nearly far enough. Correlaion present in data does not even indicate a real world relationship at all!</p>
<p>Point 2 can be demonstrated.</p>
<p>Let’s start by generating two random normal distributions of size 20, <span class="math inline">\(\mu\)</span> is 0, <span class="math inline">\(\sigma\)</span> is 1.</p>
<pre class="r"><code>set.seed(123)
dist1 &lt;- rnorm(20)
dist2 &lt;- rnorm(20)</code></pre>
<p>These two distributions, aside from being generated on my computer have nothing to do with each other and represent nothing in the real world. If you have no interest in the R code, that’s fine, just pay attention to the fact that we’re working with random distributions that have no real world relationship.</p>
<p>We can correlate them.</p>
<pre class="r"><code>cor(dist1, dist2)</code></pre>
<pre><code>## [1] -0.09172278</code></pre>
<p>The Correlation is -.09, not very large.</p>
<p>Let’s do this again.</p>
<pre class="r"><code>set.seed(9201989)
dista &lt;- rnorm(20)

distb &lt;- rnorm(20)

cor(dista, distb)</code></pre>
<pre><code>## [1] -0.3213314</code></pre>
<p>Hm, -.32, we’re starting to get somewhere. But they’re random numbers. Let’s say we do this over and over again and graph the correlation coefficients. I created a function to do this as an exhibit. It takes as input, the number of times to replicate creating distribrutions of size <em>samples</em> and a particular correlation value, or <em>cor.cutoff</em>. The function returns the number of times a correlation value above this cutoff is returned. The code is below. It returns a list. Element 1 is the number of times a correlation value at least as large as cor.cutoff is returned. The second element is a line plot of correlation coefficients. Each replication returns one correlation coefficient. And the third element in the returned list is a dataframe containing each correlation coefficient for each replication.</p>
<pre class="r"><code>library(ggplot2)

totfunc &lt;- function(reps, samples, cor.cutoff){
random &lt;- function(samples){
  dist1 &lt;- rnorm(samples, 0, 1)
  dist2 &lt;- rnorm(samples, 0, 1)
distab &lt;- as.data.frame(cbind(dist1, dist2))
x1 &lt;- cor(dist1, dist2)}


test1 &lt;-replicate(reps, random(samples))

test2 &lt;- as.data.frame(cbind(test1, c(1:reps)))

corpl &lt;- ggplot(data = test2, aes(x=V2, y = test1)) + geom_line() + xlab(&quot;Replication Number&quot;) + ylab(&quot;Pearson&#39;s Correlation Coeffient Value&quot;)
x2 &lt;- sum(test2$test1 &gt;= cor.cutoff)

corpl
plotsr &lt;- list(x2, corpl, test2)
return(plotsr)
} </code></pre>
<p>Let’s see how this works. We’ll start with distributions of size 20 and correlate them. We’ll repeat this process 1000 times. we want to know how many correlation coefficients above the absolute value of .6 exist.</p>
<pre class="r"><code>set.seed(23)
testtot &lt;- totfunc(1000, 20, abs(.6))</code></pre>
<p>To see the plot of correlation coefficients:</p>
<pre class="r"><code>testtot[[2]]</code></pre>
<p><img src="/post/2018-09-10-on-measurement_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Not the pretties, but you get the idea. Correlation values are all over the place. How many times does the correlation value exceed the absolute value of .6?</p>
<pre class="r"><code>testtot[[1]]</code></pre>
<pre><code>## [1] 5</code></pre>
<p>Yep, 5 times. That’s fairly impressive. We were able to generate two random distributions, and depending on the sample, got correlation values that exceeded .6.</p>
<p>If you want to understand why a correlation coefficient doesn’t necessarily describe a real world relationship, this is why!</p>
<p>And if you want to play with some other numbers, say, with random distributions of size 1000 and do it a thousand times, we can.</p>
<pre class="r"><code>test2 &lt;- totfunc(1000, 1000, abs(.6))

test2[[1]]</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>test2[[2]]</code></pre>
<p><img src="/post/2018-09-10-on-measurement_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
