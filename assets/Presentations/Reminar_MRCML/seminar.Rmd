---
title: Differential Item Functioning Detection Problems - Why is it So Hard? </span>
author: Danny
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      titleSlideClass: ["left", "middle", "inverse"]
      highlightStyle: agate
      highlightLines: true
      countIncrementalSlides: true
---


<style>
pre {
  white-space: pre !important;
  overflow-y: scroll !important;
  max-height: 50vh !important;
}
</style>

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_knit$set(root.dir ="~/Rprojects/danielbkatz.github.io/assets/Presentations/Reminar_MRCML")
                       

```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
library(RefManageR)

mono_light(base_color = "#29132e",
          white_color = lighten_color("#29132e", 0.7),
          black_color=darken_color("#29132e", 0.3),
 code_highlight_color = "#686862",
 link_color = "#4C4D41",
 #code_inline_background_color = "#A6A6BA",
 code_inline_color = "#02191c",
 code_font_google   = google_font("Droid Mono"),
 #background_image = "methodsu.jpg",
 background_size = "15%",
 background_position = "bottom left")
```

```{r, include = FALSE}

library(tidyverse)
```


```{r, load_refs, include=FALSE, cache=FALSE}

library(RefManageR)

BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "alphabetic",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)

myBib <- ReadBib("ref.bib", check = FALSE)
```
---
class: middle 

# What I'll be trying to do here

+ Discuss my interests in the way we talk about uncertainty in statistics (esp. in social science)
  
--
  
+ We talk about a lot of different types of "error" that are actually very different
  
--

+ We present and handle this error as a unified concept - I think doesn't help us advance

--
  
+ Can we generate a framework to make it easier to talk about different types of uncertainty?


---

class: middle 
  
# Moving forward

#### Motivating Examples 

+ How different types of uncertainty rears their ugly heads in ways we don't treat as measurement error but is uncertainty (Factor score indeterminacy and scale indeterminacy)

--

+ Review of the Generalized Linear Model (GLM) with "normal" assumptions to introduce IRT (we'll stick with the 1PL model for now)

--
  
+ DIF detection in IRT history and problems - what's uncertainty got to do with it?

--
  
+ Proposed Solutions to the DIF detection problems

--

+ what role this may all play in my dissertation (maybe)


---

class: middle

## Thinking about attempts to measure (could make this more generic or more specific)

.center[
![data generating process](Process.png)
]


---

class: middle

# Lots of sources of uncertainty 


+ What are we measuring? What's the estimand/measurand? (is our instrumnet only measuring this?)

--

+ What person features (knowledge, skills, abilities, atttitudes) are relevant and irrelevant?

--

+ What is the variance of the results due to instrumental reasons (i.e. particular item used)?

--

+ Sampling distribution and the variance of the estimator?

--

+ Where is measurement taking place? Responses to items? Or in the transformation/mapping of (categorical) item responses to a continuous (or just different) space?

--

These sources of uncertainty add up and we have no great way of updating this uncertainty (i.e. - "I figured out that this instrument reduces").

  
---
class: middle
We're pretty good at making sure we do not differentiate among different distributions (distributions of what?)

Example: As cited in  `r Citet(myBib, c(9), .opts=list(cite.style = "authoryear"))`

> "In a subsequent figure (Lord & Novick, 1968, Figure
16.11.1, p. 380) they plot a second kind of normal density
that is the distribution of q in the population (that Lazarsfeld
had referred to as f(x)); that distribution is shown as
the dashed curve in the lower panel of Figure 1. This representation
transforms Thurstone’s (1925) story into a fullyfledged
statistical model, distinguishing between the population
distribution and the response process variable, both of
which, confusingly, are Gaussian."

---
class: middle

## How does definitional uncertainty effect our typical measurement models? (I think...)

+ First general case: Factor score indeterminacy

Factor model (for one person): $y_p = \lambda_{pk}f_k + u_p$ 
  
  
Where $y_p$ is the observed response for item $p$ (could add a subscript for person), $\lambda$ is (a vector or matrix of) factor loadings for item $p$ and factor $k$ and $f_k$ is a factor or factor score. $U_p$ unique factors. 

--
  
+ We get a model implied var/covar matrix  

+ We get parameters (slopes/loadings, observed and latent var/covar, that are not affected by score indeterinacy).

However...

---
class: middle

# Factor Score indeterminacy

As cited in  `r Citet(myBib, c(8), .opts=list(cite.style = "authoryear"))`

> In simplest terms, “[f]actor indeterminacy is the inability to determine uniquely the common and unique factor variables of the common factor model from the uniquely defined ‘observed variables’ because the number of observed variables is smaller than the number of common and unique factors” (Mulaik & McDonald, 1978, p. 177). 

An infinite number of factor scores may be used to generate the stated parameters on the previous slide.
---
class: middle

# Scale indeterminacy

+ In IRT, we imagine "person abilities" (intercepts) and "item difficulties" (thresholds in the factor model) (in the 2pl, we'd have slope or discrimination parameters equivalent to the factor model).

--

+ The values of person abilities and item difficulties are in an arbitrary unit - let's say logit or probit.

--

+ No unit of measurement in logit or probit (think of odds as a ratio - the same ratio can be achieved with an infinite set of values)
  
--
  
+ To detect differential item functioning (to be defined) - we need some scale so we can compare item response probabilities, matched on abilities


+ However, to figure out which items have DIF, you need to know which items do not have DIF for comparison - but you need to know which items do have DIF! A problem! Again, definitional problems


---

class: middle 

## Why is this the case?

+ Definitional uncertainty: We are working with "latent variables" - if we had more confidence in what we were measuring - we'd probably not work with latent variables. 

--

+ It makes it harder to define scale anchors (i.e. freezing or boiling point of water)

--

+ Hence, creating units and identifying our models is quite hard. 

--

+ Can statistics helps us out? Let's back track and run through the IRT model.


---
class: middle 


# The Basics and the Generalized Linear Model

Classic example, borrowing from Andy...

.pull-left[

+ Let's say I tell you that the average time it takes to get to the store from UCSB is 20 minutes. That's all I tell you.

+ When you go to the store (any store), how long would you guess your journey is about to take?


]
--

.pull-right[

+ We can write, $\mathbb{E}[Time] = 20$ minutes


+ Now, let's say you're on some sort of schedule...


+ You might also be worried about the variance of these trips (you might want to know about the probability of the trip taking more than 30 minutes)

]

---

class: middle 


# The GLM, cont'd

+ To account for this, you request I describe your trip times with a distribution, namely, a normal distribution(this would have to be some special forms of the gaussian distribution constrained to positive values but let's ignore)

+ These trip times now have a mean and variance to describe expectations and uncertainty

+ $Time \sim  \mathcal N(\mu = 20, \sigma^2 = 25)$

+ We can plot this


---

```{r fig.align='center', fig.retina=3, fig.width = 9, message=FALSE, warning=FALSE, echo=FALSE, figh.height=7, paged.print=FALSE}

p1 <- ggplot(data = data.frame(x = c(0, 40)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 20, sd = 5)) + ylab("Density") +
  xlab("Time in minutes") +
  scale_y_continuous(breaks = NULL)

p1

```

Note, again, this would have to be truncated at zero. And presumably - trip time to Trader Joe's or whatever is finite unless you leave UCSB at 5PM.


---

class: middle 


# Let's write this as a linear model now (note, this is a "theoretical model" no data collected)

 $$y = \beta_0*X_0 + \epsilon$$
--

$X_0$ is just defined to equal 1

+ $\epsilon$ is normally, independently, and identically distributed with mean 0 and variance - let's say the mean is also 20 minutes

--

+ $\mathbb{E}[Time] = \mu = \beta_0 = 20$ minutes

+ (remember that an arithmetic mean, in this case, minimizes the squared error or $argmin\{(\hat{\beta}_0-y)^2\} = \bar{y}$)
---

class: middle

# Running this model

In R this model looks like
```{r eval=FALSE}

lm(data = df, y ~ 1)

```

+ The model now has a "linear" or deterministic part, and a "random" part - that the outcome will never exactly equal the mean ("the error distribution")
---

class: middle

# The GLM cont'd

Rewritten in matrix form - this model looks like:

$$\hat{y} = X\beta$$
or...

$$\mathbb{E}[Time] = X\beta$$
---

class: middle

# Representing this model



.pull-left[
+ Using the notation of `r Citet(myBib, 1, .opts=list(cite.style = "authoryear"))`


![simple_random](simple_random.png) 

]
--
.pull-right[
+ The squiggly line I tried to draw (sorry...) represents the random component of the model - the uncertainty, our error or (un)conditional distribution

+ Observed time is not the same as the linear model predicted time - there's some uncertainty 

]

---

class: middle

# The GLM continued

.pull-left[

+ What I didn't tell you is that the average trip takes 20 minutes by bike ... 

+ But it actually takes 5 minutes less time by car (on average)

+ Now we have additional information - but still need a distribution for modeling each mode of transportation!

$$\mathbb{E}[Time|Mode] = X\beta$$


]

--

.pull-right[

+ The distribution is now _conditioned on_ each mode of transport - the conditional distribution

  + Normally distributed error, mean 0.
  
  + Two normally distributed populations - mean 20, and mean 15, but same variance. 
  
  + Special model to fit different variances but no longer in the OLS realm.
  
]
  


---

class: middle

# The GLM continued

+ Given the description above, in non matrix notation, with bikes as reference group

$E[Time|Mode] = 20 * X_0 + (-5*X_1)$

 + Note - I've done something pernicious - I've switched between the empirical/frequency distribution of the observed data and the abstract/ideal distribution (namely, the Normal)
---
```{r fig.align='center', fig.retina=3, fig.width = 9, message=FALSE, warning=FALSE, echo=FALSE, figh.height=7, paged.print=FALSE}

df <- data.frame(x = sample(c(0, 1), size = 1000, replace = T))

df$y <- 20 + df$x*-5 + rnorm(100, 0, 5)

lm_fit <- lm(data=df, y ~ x)

# x <- x - mean(x)
# y <- y - mean(y)


# For every row in `df`, compute a rotated normal density centered at `y` and shifted by `x`
k <- 2.5
sigma <- sigma(lm_fit)
ab <- coef(lm_fit); a <- ab[1]; b <- ab[2]

x <- seq(-k*sigma, k*sigma, length.out = 100)
y <- dnorm(x, 0, sigma)/dnorm(0, 0, sigma) * 2

x0 <- 0
y0 <- a+b*x0
path1 <- data.frame(x = y + x0, y = x + y0)
segment1 <- data.frame(x = x0, y = y0 - k*sigma, xend = x0, yend = y0 + k*sigma)
x0 <- 1
y0 <- a+b*x0
path2 <- data.frame(x = y + x0, y = x + y0)
segment2 <- data.frame(x = x0, y = y0 - k*sigma, xend = x0, yend = y0 + k*sigma)


ggplot(df, mapping = aes(x=x, y=y)) + geom_jitter(color="#b50c1140") + 
  geom_smooth(method='lm', se=FALSE, color="black") + 
  geom_path(aes(x,y), data = path1, color = "#a52a2a") + 
  geom_segment(aes(x=x,y=y,xend=xend,yend=yend), data = segment1) +
  geom_path(aes(x,y), data = path2, color = "#b50c11") + 
  geom_segment(aes(x=x,y=y,xend=xend,yend=yend), data = segment2)


```


---

class: middle

# Binary or categorical outcomes? 

+ We can't assume our conditional distribtions are Normal anymore  - but... 

+ The outcome data might have a bernoulli, binomial, poisson, exponential distribution which are all part of the same family of distribution

+ This common family of distributions is known as the exponential family of distributions - the lifeblood of the GLM.

---

class: middle

# The common family for the GLM - The exponential family

+ When you write these conditional distributions in this exponential family form:
  + Given a linear function/expectation - such that, $\eta = B_0 + B_1*X_1..$ 
  
  + There is a function $g(\mu) = \eta$ (a function links our expectation to the linear portion of the model)
  
  + A function can be inverted so $g^{-1}(\eta) = \mu$
  
--

When this parameter/function comes directly from the distribution written in the common exponential family form, this is called the "cannonical parameter" such that

--

$\eta \equiv\theta$ & $g^{-1}(\theta)$ will get you $\mu$

--

For a linear regression, with normal assumptions, g() is the identity (like multiplying by 1)

---

# For logistic regression

The outcome is defined as being drawn from a Bernoulli (or Binomial): [Click Here for my Derivation](https://dbkatz.com/assets/Presentations/Reminar_MRCML/linkfun.pdf)

.pull-left[
$Y_i \sim Bern(p)$.

The mean of the Bernoulli distribution, or the expectation, is the probability of one of the two outcomes (coded as 1 or 0) which we'll call `p`.

Conditional expectation is:

$$P(y=1|x)$$
]
--
.pull-right[
$$g(\mu) = log(\frac{p}{1-p})$$


The inverse ...

$$p = \frac{exp(\eta)}{1+exp(\eta)}$$
]




---

class: middle 

# Ok, enough of this...

+ The point isn't to memorize this - it's to see the similarities in item response models 

+ Item response data is comprised of categorical data 

+ While avoiding calling categorical data "normally distributed" because the middle categories have the highest frequencies 

---

class: middle

# Symbolizing our model

We add a straight line between the linear model and eta (they're equivalent) and now we add a link function which is symbolized by the straight arrow between $\eta$ and the probability.

+ We still have the random component (of course)

.center[

![link](link_fun.png)
]

---

## The Rasch or 1PL model

From De Boeck & Wilson (2004):

.pull-left[

$P(Y = 1 | \theta, \delta) = \frac{exp(\theta-\beta)}{1+exp(\theta-\beta)}$ or   

$P(Y = 1 | \theta, \delta) = \frac{1}{1 + e^{-(\theta-\beta)}}$

![rasch](rasch_diagram.jpg)

+ $Y_{pi}$: response of person p on item i (categorical)
]

.pull-right[

+ $\beta_i$: item or item category difficulty - it'll be a vector

+ $X_{ik}$: design matrix containing 1s and 0s pertaining to if the particular $\beta$ corresponds to item i in category k

+ $\theta_p$: the person ability - it is a random effect (next slide)

+ $Z_{p0}$ is a constant (here) but later a set of predictors - basically showing that $\theta_p$ is sampled from a distribution - a random effect

]

---
class: middle

# Formally:
Since item responses are now not independent, we need to deal with the clustering within person...

One way - 

1. $$\pi_{pi} = P(Y_{ik}=y|\theta_p, \beta_{ik})$$
--

1. $$\pi_{pi} = \frac{exp(\theta_p-\beta_{ik})}{1 + exp(\theta_p-\beta_{ik})}$$
--

1. $\theta_p \sim N(0, \sigma^2_\theta)$ - but other functional form of $g(\theta)$ could be used.
  
This last line identifies the model and anchors the scale - first source of uncertainty. Less scale indeterminacy than a 2PL+ model with extra parameters. 

- Known as th Multidimensional Random Coefficient Multinomial Logit model (MRCML)


---
class: middle

### Writing the Rasch model as a linear model

+ $\eta_{pi} = \sum_{p=1}^n\theta_pZ_{pj} + \sum_{k=1}^K\beta_{ik}X_{pik}$

+ Not pretty, but we just wrote this out as a linear model like you're used to. But $\beta$ is now reversed sign - this is the item "easiness" instead of difficulty.

+ $X$ is a design matrix of item predictors

+ $Z$ is a design matrix of person predictors - $\theta$ and sometimes its predictors might be modeled as random effects.

Matrix notation: 
$$Y = Z\theta_p + X\beta + \epsilon_p$$
---
class: middle
# GLMM Cont'd

Matrix notation: 
$$Y = Z\theta_p + X\beta + \epsilon_p$$


+ We can now use design matrices with dummy variables, effectively, in the Rasch model

$$P(Y_{ik} = 1; Z , X | \beta, \theta) = \frac{exp(z_{ik}\theta + x_{ik}\beta)}{\sum_{k=1}^{K_i}exp(z_{ik}\theta + x_{ik}\beta)}$$

+ z and x are person and item specific elements of the design matrices `Z` and `X`

+ this form of the model allows for all sorts of item types...

Item difficulties are defined as the point along the ability continuum a person has a 50% chance of getting item correct.


---
class: middle


```{r  fig.align='center', fig.width = 9, message=FALSE, warning=FALSE, echo=FALSE, figh.height=4, paged.print=FALSE}
library(ggplot2)

f_x <- function(x){plogis(x-2)}


ggplot(data = data.frame(x=-4:4), aes(x = x)) +
  geom_function(fun=f_x) +
  labs(x = "Person Ability inLogits", 
       y = "Probability of response in category k",
       title = "Item Characteristic Curve for Item with Difficulty of 2 logits ")

```



---

class: middle

# In lme4

I'd recommend using IRT-specific packages/software for basic models - much faster

Data would be in "long form" - 
```{r eval = F}
library(lme4)

mod1 <- glmer(response ~ -1 + item_id + (1|person_id), 
              family = binomial, data = df)

```

---

class: middle

# Predictors in the Rasch model? Why?


+ You want to know about the influence of item property Y (word problem vs non-word problem, specific word use on a self-report survey) 

--

+ Differential item functioning - is an item (category) relatively harder (to endorse) for certain student groups even after matching on the level of the property of interest?


$$P(Y=1 |\theta_p, \beta_i, group_p)) != P(Y=1|\theta_p, \beta_i)$$

--

+ There are person properties that you want to account for or know about (being in a specific classroom or not)?

--

+ Some people would reject the MRCML as a Rasch model due to distributional assumptions about the person parameter (ussing MML estimation instead of CML)

--

+ As you can see, the concept of DIF gets really complicated pretty quickly
---

class: middle

# DIF 

+ DIF can be conceptualized as unintended, group specific multidimensionality  
  
--
  
+ Uncertainty related to group membership - often grouping variables are really proxies for things we care about but maybe cannot specify - hinting at the need for either multidim models or mixture models
  
--
  
+ DIF is an interaction between each item and each person group - a lot of tests! 
  
--

+ Need to have some items that do not have DIF 
  

--

+ Observed variable approaches (based on matching on sum scores, for instance - more like prediction invariance)

--

+ Latent variable approaches - matching on the latent variable or similar
--

---
class: middle

# IRT traditions of DIF
+ Estimate models seperately in each group - plot item parameters against each other from each group. Deviation from 45 degree line is evidence of DIF

(skipping Mantel Haenzel methods)
--

+ Note - if we split up people into high and low ability - and do the above
  + If we find that people of lower ability have a higher probability of answering an item correctly than they should relative to people of a higher group then Rasch tradition will flag for DIF while 2PL will fit!
  
  + We're in a really fuzzy area now! 
  
---
class: middle

# DIF cont'd

+ Another subset of approaches constrains the parameters of items in one group, and frees in another (this gets messy with many groups)

+ But since we have scale indeterminacy, not clear which should be constrained

+ Iteratively treat items as if they do not have DIF

---
class: middle
# The interaction approach - item "purification"

+ The interaction approach: Checking to see if items or item categories are relatively harder (or easier) for students of different subgroups at the same "level" of the property via interaction between person and item


We can express the model: 

$$\eta_{ip} = \theta_p + \beta_{ik} + \beta_1*group_p + \gamma_i(item*group_p)$$



```{r eval = F}
library(lme4)

mod1 <- glmer(response ~ -1 + item_id*group + (1|person_id), 
              family = binomial, data = df)

```
---
class: middle

# 2PL (in FA parameterization) DIF Model (parameterization from the paper):
.pull-left[
2pl
+ $P(Y_{pi}|\theta_p) = \frac{1}{1+e^{-(c_{i} + \alpha_{i}\theta_p)}}$

+ $c_i$ = intercept of item i

+ $\alpha_i$ = slope parameter

+ if slope parameters the same (and 1) then 1PL model
]

.pull-right[
DIF model
+ $P(Y_{pi}|\theta_p) = \frac{1}{1+e^{(c_{ig} + \alpha_{ig}\theta_j)}}$

+ $g$ is group membership indicator

+ Each group can have its own mean and variance - identification constraints necessary

+ Fix mean and variance of one group and constrain select a few item difficulties and slopes to be equal

+ Red flag...
]

---

class: middle

## Reg DIF

+ $P(Y_{pi}|\theta_p) = \frac{1}{1+e^{-(c_{0i} + c_{1i}x_i) - (\alpha_{0i}+\alpha{1i})\theta_p)}}$

+ $c_0$ and $\alpha_0$ represent reference/baseline vals, x is a group membership var

+ other parameters represent difference between reference and contrast groups. 

+ lasso penalty: $$l(\gamma)Reg_{DIF} = l(\gamma)_{DIF}-\tau\Sigma_{i=1}^p[|c_{1i}| + |\alpha_{1i}|]$$

+ $\gamma$ are model params - or Marginal LL of the model maximizing with respect to $\gamma$

---
class: middle

# What does Reg dif help solve?

.pull-left[
+ Uncertainty due to multiple testing

+ Does not require anchor items

+ Lower type 1 error 

]

.pull-right[
What problem remain:

+ Scale indeterminacy

+ Sampling error

+ identification solved by Reg dif so only sort of a solution

+ Constant DIF across people-group-item interactions (uniform)

+ selecting model/tuning parameter based on fit

+ still select items based on p-values

]




---
# Finally some new territory

+ It is typical to have items as fixed effects in in the 1pl model. But...

+ Random Items, Random Persons, random groups for instance, also plausible:

```{r eval = F}
library(lme4)

# Random Item and Persons - treat items as random samples
mod1 <- glmer(response ~ -1 + (1|item_id) + (1|person_id), 
              family = binomial, data = df)


# DIF random across persons - individual differences of DIF -correct?
random_DIF <- glmer(response ~ -1 + item + (1 + group*item_id|person_id), 
              family = binomial, data = df)

```

+ DIF is somewhat "regularized" now - shrunk to zero - hinting at the necessity for mixture models?

+ For all DIF models - DIF is somewhat exploratory - stat. significant interactions considered to be DIF items favoring one group or another.

---
## Building on this model ... 

+ Random Item Mixture model approach - Instead of looking for or providing anchor items...
--

+ A mixture model of item difficulties is used:
 + A DIF class of items - items that have different difficulties across groups
 + A non-DIF class of items - items that have the same difficulties - anchor items
 
 
 --
   
+ Mixture model is probabilistic - provide a starting point for the iterative procedures discussed

--

This is the Random Item Mixture model (RIM)


---
class: middle
# RIM model:

 `r Citet(myBib, c(10), .opts=list(cite.style = "authoryear"))`:
> "For the DIF class distribution on the other
hand, a G-dimensional distribution (where G equals the number of person groups)
is used such that each person group can have its own item parameter."

For two groups:
$$
\begin{equation}
    C_j =
    \left\{
        \begin{array}{cc}
                0 & \mathrm{if\ item\ j\ shows\ no \ DIF}\\
                1 & \mathrm{if\ item\ j\ shows \ DIF} \\
        \end{array} 
    \right.
\end{equation}
$$

---

class: middle
## How does this manifest?


For non - DIF
$$logit[Pr(Y_{ijg} = 1 |C_j = 0)] = \theta_{ig} - \beta_j$$
$$B_j|C_j \sim N(\mu_{\beta}, \sigma^2_\beta)$$

---

class: middle

FOR DIF...oi vei  
$$logit[Pr(Y_{ijg} = 1 |C_j = 1)] = \theta_{ig} - \beta_{jg}$$

item difficulty sampled from G-Variate normal distribution:  


$$

  \begin{bmatrix}
      \beta_{j1} \\
      \beta_{jG} \\
    \end{bmatrix}\Bigg\vert C_j = 1 
    \sim N
    (\begin{bmatrix}
     \mu_{\beta_1}\\
     \mu_{\beta_G}\\
  \end{bmatrix} \Sigma_\beta)
$$

---

class: middle 

## Potential Applications of Random Item/Person DIF

+ A scenario where observed groups are languages spoken at home
--

+ A test of reading strategy uses (for instance) - formative - used for making readig groups

--

+ We have students who read like "Spanish speakers" to varying degrees who are both in the observed Spanish speaking group and observed Spanish speaking group

--

+ Use RI DIF to characterize the extent to which items have varying DIF effects for different students - characterizing their "level" on the DIF dimension

--

+ When selecting among items and choosing who to give them to...would a RE version of DIF be more informative?

--
+ Can we combine the RI and RIM model to some extent more? 


---
class: middle

# Different approaches: Regularization/Lasso

+ To handle the multiple testing and anchor item problems - shrink coefficients toward zero.

--

+ Add a tuning/penalty parameter to the model such that $$\eta_{ip} = \theta_p + \beta_{ik} + \beta_1*group_p + \gamma_i(item*group_p)$$ 
 + shrink $\gamma_i$ toward zero
 + for instance -penalize the DIF parameters via a tuning parameter $\tau$ - $\tau\Sigma_{i=1}^n |\gamma_i|$ (or something)

--

+ But is this more informative than RI approach to DIF or only an error control type thing?



---

# It would be great if...

We could have some form of the model so it looks the same regardless of ...

--

+ Item type (dichotomous, polytomous)

+ Item covariates

--
+ Dimensionality (single or multidimensional, within or between item dimensionality)

--

+ Person covariates included (get person abilities or person property effects on abilities)

--

+ Rater effects included
--

+ That is the Multidimensional Random Coefficients Multinomial Logit Model (MRCML)

---

class: middle

# Rasch Model revisited using the design matrices



---
class: middle

## Partial Credit Model - Three Response Options


1. $$P(x_i =0) = \frac{1}{1 + exp(\theta-\beta_{i1}) + exp(2\theta-(\beta_{i1}+\beta_{i2})}$$

1. $$P(x_i=1) = \frac{ exp(\theta-\beta_{i1}) }{1 + exp(\theta-\beta_{i1}) + exp(2\theta-(\beta_{i1}+\beta_{i2})}$$

1. $$P(x_i=2) = \frac{ exp(2\theta-(\beta_{i1}+\beta_{i2}))}{1 + exp(\theta-\beta_{i1}) + exp(2\theta-(\beta_{i1}+\beta_{i2}))}$$


---

## Design Matrices

$$P(X_{ik} = 1; A, B | \delta, \theta) = \frac{exp(x'(b_{ik}\theta + a'_{ik}\delta)}{\sum_{k=1}^{K_i}exp(z'(b_{ik}\theta + a'_{ik}\delta))}$$
.pull-left[
B = 
$$
\begin{bmatrix}
0 \\
1 \\
2 \\
\end{bmatrix}
$$
A = 
$$
\begin{bmatrix}
0 & 0 \\
1 & 0 \\
1 & 1 \\
\end{bmatrix}
$$

]

.pull-right[
+ Person responds in category 2 (the highest category - they get a dummy variable of 1 for the third category/row of the item in the x matrix)

+ Numerator - invokes the 2 in the B design matrix - so we start with $2\theta$

+ Numerator - invokes the third row of the A matrix - the 1 and 1, so we get $\beta_{i1} + \beta_{i2}$
]

---
## `Stan`

+ Stan allows ultimate flexibility but is complicated to use 
--

+ Fully Bayesian -  takes a while but you can really express models as you'd like  
  
--  

+ There is a large and growing R eco system being built around Stan  

--

+ A package called `edStan` built by Daniel Furr (a student of Sophia Rabe-Hasketh)  

--

+ There are case studies in the Stan manual for IRT models in R

--

+ Can express any model so long as its probabilistically sound - `Stan is a probabilistic programming language`

---

## Example - express the Rasch model in a Bayesian way

"hyperprior for persons"
$$\theta \sim N(0, \sigma^2)$$

"hyperprior for items"
$$\delta \sim N(0, 1)$$

"prior for $\sigma$"
$$sigma \sim exp(.1)$$


"Likelihood"
$${Pr}[y_n = 1] = {logit}^{-1}(\theta - \delta)$$

$$ y \sim bernoulli(Pr[y_n=1])$$
---


# References

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(myBib, start = 1, end = 6)

```

---

```{r refs2, echo=FALSE, results="asis"}
PrintBibliography(myBib, start = 7, end = 10)

```

