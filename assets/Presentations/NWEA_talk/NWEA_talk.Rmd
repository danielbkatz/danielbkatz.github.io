---
title: Construct Definition
subtitle: Tools from Philosophy and Psychometrics 
author: Daniel Katz
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      titleSlideClass: ["center", "middle", "my-title"]
      highlightStyle: agate
      highlightLines: true
      countIncrementalSlides: false

        
---
class: middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}

library(xaringanthemer)
mono_light(base_color = "#29132e",
          white_color = lighten_color("#29132e", 0.7),
          black_color = darken_color("#29132e", 0.3),
 code_highlight_color = "#686862",
 link_color = "#4C4D41",
 code_inline_background_color = "#A6A6BA",
 code_inline_color = "#02191c",
 code_font_google   = google_font("Droid Mono"),
 #background_image = "methodsu.jpg",
 background_size = "15%",
 background_position = "bottom left",
)


```


class: middle

## Outline

1. Brief Overview of Motivations and Interests - construct definitions, ontology, and psychometrics

2. Presenting an iterative workflow - between theoretical considerations and (statistical) modeling and back

3. Concluding thoughts



```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(caTools)
library(gganimate)
library(knitr)

```


---


class: middle, center

# Part 1 
## Motivations and Interests

### Trying to sort out what our definitions "do"



  
---
class: middle

# How I got here 

1. Early interests: literacy and measures of reading ability and/or text difficulty

--

2. I was starting to work on an in-progress formative measure of `Reading Strategy Use` 
  + Called the Strategy Use Measure (SUM)
  + Attempt to measure four distinct strategies hypothesized to be useful for students encountering challenging texts

--

3. It was going to start being used in the classroom (formatively) - but teachers were certainly going to compare it to higher stakes reading assessments


---

class: middle

# How I got here - Definitions of Reading


The National Assessment of Educational Progress (NAEP) - reports on student reading comprehension ability across the United States

Defines reading ability as (with descriptions of each):
+ Understanding written text. 
+ Developing and interpreting meaning. 
+ Using meaning as appropriate to type of text, purpose, and situation.



---

class: middle

# Controversy


+ What are the necessary "causal" antecedents to reading ability within NAEP?

+ Is background knowledge part of `reading comprehension` (switch in terms)? 

+ For the 2025 framework, there's been vigorous debate




---

class: middle

## Fuzzier example - Measuring resilience

+ Project with school psychologists wanting to construct a culturally sensitive measure of resilience

+ I (along with my advisor) were the "measurement folks" on the team

+ Definitional disagreements among the school psychologists and the literature



---

# Paraphrasing different positions

.pull-left[
_Resilience as life outcome:_

+ May invoke a between-person descriptive view of resilience 

+ Not specific enough to figure out if this outcome should be continuous or dichotomous
]

--

.pull-right[

_Resilience as internal:_

+ Resilience defined by attitude, belief, or propensity when encountering a challenge

+ May invoke a within-person, explanatory view of some phenomenon
]

--

But there is no **correct** definition! Resilience is a metaphor.

---

class: middle

## Then what?


+ I wanted to create sort of a warning labels for myself (a cheat sheet if you will)

+ I figured I'd find one but they mostly existed in philosophy of language texts (with a few exceptions)

+ At first, it was just for me to sort out the work I was doing...

???

Since then, I've wanted to map out the ways that we (psychometricians, educational and psychological researchers) might be bewitched by our words



---

class: middle

## Defining measurable properties



+ We name (signifier) the properties we want to measure ("reading ability") - so our words refer to something (the signified)

+ Thinking this way helps us figure out what it is to be these things we'd like to measure (ontology)

+ Allows us to push and refine our definitions




---

class: center



![conflations](reading_abil_designator.png)



---

class: middle

# So I began to wonder...

1. What should a properly specified definition look like?

2. What is the role of definition in measurement activities? Who gets to do that?

3. Is defining . . . 
  +   a language-based enterprise?
  + an empirical one?
  + or some combination? 
  
4. Given any of these, what should count as enough for "measuring?”


---


class: middle, center

### Diagramming the measurement process




![meas_process](meas_process.png)

---

class: middle

## What are common features of measurement?

`Objectivity` or `“object-relatedness”`: the measuring instrument is “sufficiently” sensitive to the property intended to be measured 
 
--

`Intersubjectivity` or `“subject-independence”`: measurement results are understood in the same way across time, place, and persons

For more, see for example (Maul, Mari, & Wilson, 2019). 

???
Objectivity = reduction in uncertainty and also helps figure out what's uncertainty related to the instrument. But these things are never perfectly certain...

---

class: middle 

### "Measurements are full of uncertainty" 

Uncertainty as a state of incomplete knowledge
  + Uncertainty about what?
  + Why is there uncertainty?
  + How do we reduce this uncertainty?


--

I'm especially interested in the concept of `definitional uncertainty` about the property of interest ("reading ability", "math ability", temperature)

--

+ Definitional uncertainty - there's only a finite amount of detail that can describe the property of interest

---

class: middle

## Some terminology


 + The [International Vocabulary](https://jcgm.bipm.org/vim/en/2.27.html) of Metrology (VIM; metrology is the study of measurement) defines definitional uncertainty:

> "component of measurement uncertainty resulting from the finite amount of detail in the definition of a measurand"



---

class: middle 

# NAEP concludes:

>For this reason, the definition of reading presented earlier 
should be considered as a guide for the NAEP Reading Assessment, not as an inclusive definition of 
reading. The definition pertains to how NAEP defines reading for the purpose of this assessment. 

From *Reading Framework for the 2019 National Assessment of Educational Progress*, p. 4

This is a restatement of the idea of definitional uncertainty


---

class: middle

# Definitions and psychometrics

Why is this relevant to psychometrics (or validation work generally)? 
  

+ What we consider relevant to the construct of interest (or not - "construct irrelevant variance") leads to items


+ Requires delimiting - "what's not part of the construct/property of interest?"

???



We have special names in psychometrics when we're measuring something other than intended 



---

class: middle 

### Definitional uncertainty as a matter of fairness


+ What's construct relevant - might lead to different models (statistical or substantive)

--

+ Different definitions of properties may lead to different student rankings
  
--
  
+ Deciding to exclude/include certain items --> test impact

--

+ Clear definitions might be demanded by learning communities


???

Sometimes we're selecting the property to measure via definition instead of via the thing we're interested in. Construct/entity conflation.

---

class: middle

# Overall...

Understanding of measurement results requires that we **collectively** trust and agree on what’s being measured (or estimated)


---

class: middle


# Some starting recommendations:

When we define - what sort of definition are we working with?

--

+ `Stipulative definition` - I, or we, claim a specific meaning of a term. Researcher dominant

+ `Use-based definition` - Meaning comes from use; requires research into collective understandings. More democratic. 





---

class: middle

# Semantics

As Fiona Hibbard says (2019, p.31): 

> “Although we use words to propose or state what kind of thing we think it is, a definition references the what-it-is-to-be that kind of thing — its principal features or structure — in order to delimit it from other kinds and to make possible a systematic study of it and its connections.”


???

For now, this effort has been a categorization exercise - trying to figure out conceptual practice that can help us reveal some confusions

---
class: middle, center

# Part 3

#### Brief case study using latent variable models


---
class: middle

# Black boxes don't define


.pull-left[
+ Latent variables in latent variable models - don't confront the measurement and definition question. 

+ "Ability" - whatever it is that makes it possible to do something

+ But also serves as explanation for variation
]

.pull-right[
![latent_var model](latent_var_1.png)
]

---


class: middle

## Opening the black box

We can take a construct modeling or construct mapping approach - (Wilson, 2005)

![construct mapping](const_map.png)
---

class: middle

Construct mapping and IRT and back again

.pull-left[

![construct mapping](const_map.png)


]

.pull-right[

#### The Rasch Model

$$P(X_{is}=1|\theta_s, \delta_i)=\frac{exp(\theta_s-\delta_i)}{1+ exp(\theta_s-\delta_i)}$$

+ item difficulties $\delta_i$

+ person abilities $\theta_s$
]
---

# But what about violations?


+ In psychometrics, we learn about differential item functioning (DIF) in sort of a causal way

+ An item is flagged for DIF when the probability of a response to an item, is not the same for people of the same ability but from different groups

+ Knowledge of tipping is thought to cause different response probabilities. We're not interested in measuring this. This is an explanatory account of DIF but helps us delimit what we're interested in measuring. 

???

People from countries without tipping as common practice have a lower probability of getting a math question about tipping correct than those people who are from countries with tipping as common practice EVEN AFTER matching on (estimated) math abilities. In metrology - this would be an "influence property" or "influence quanity"

---
## The Strategy Use Measure (SUM) - Formative, used for creating reading groups in the classroom

.pull-left[
Four correlated dimensions:

+ Morphological Awareness Items

+ Macro and Micro Relationships in Text

+ Inter-and Intra-Sentential Context Clues

+ Cognates (Spanish/English)
]

--
.pull-right[


Each dimension is thought to assess the extent to which readers are able to put these strategies to use when they encounter a difficulty text.



]
---

## Construct maps

From (Arya, Clairmon, Katz, & Maul, 2020)


.pull-left[

![Morphological](MA_construct.png)


]

.pull-right[

![Morphological](cog_construct_map.png)


]

---

Example items:

.pull-left[
### Morphological Awareness

The book covers were heterocoloreous. What does "heterocoloreous" mean?
+ **Different Colors** (1)
+ same colors
+ bright red colors
+ different red colors
]

--

.pull-right[
### Macro and Micro Relations in Text

Glaciers are very large layers of ice that move very slowly.
Which detail is least related to this sentence?   
 + **Glaciers have been long studied by scientists.** (1)
 + Glaciers can be as large as many countries.
 + Glaciers move an average of 200 feet per year.
 + Glaciers are able to carve out rock as they move.
]
---
class: middle

### Inter-and Intra-Sentential Context Clues

Tree frogs are about two `strimes` long, which is the size of your finger. They must move `pandery` to capture flies, their favorite food ... Tree frogs must escape from `morpes`, like snakes and bats, by moving quickly. _What could `pandery` mean?_

+	**quickly** (1)
+	slowly
+	gently


---


## Key items
![Qcog3](QCOG3.jpg)

![Qcog20](QCOG20.jpg)


---

## Why do we worry about DIF?


.pull-left[
### Fairness:

1. We want to treat like cases alike

2. High stakes assessment: no test taker (or group) has undo (dis)advantage

3. The AERA, APA, NCME *Standards* denote fairness as a validity issue (focusing not just on DIF)
]

--
 
.pull-right[
### Measurement, ontology
1. Make sure score interpretations are same across groups (equating, if you will)

2. Limit the extent to which instrument is sensitive to sources not related to the construct

3. Finding DIF is important for examining construct of interest
]

Of course, also legal ramifications 



---

## What we're measuring

.pull-left[

+ We name the properties - so our words refer to something 

+ The thing we use to refer to those things - the label - is a signifier

+ Thinking this way helps us figure out what it is to be these things 


]



--


.pull-right[
Forms of uncertatinty to think about...

Sometimes, we don't know what's different and what's the same thing

![jupiter](NWEA_talk_files/jupiter.png)


]
---

class: middle, center

# Example

.pull-left[

![same_entity](reading_abil_both.png)

]

--

.pull-right[

![sep_entity](reading_abil_sep.png)

]

---
class: middle 

# Bewitched by language - conceptual thinking

+ Sometimes labels are too ambiguous

+ Sometimes definitions are too ambiguous

+ But always, language is never quiiiiiiite enough. 

+ We need to delimit

---
class: middle

# Or what about empty or contingent references?

![unicorn](NWEA_talk_files/unicorn.png)



---

class: middle

# Semantics

As Fiona Hibbard says (2019, p.31): 

> “Although we use words to propose or state what kind of thing we think it is, a definition references the what-it-is-to-be that kind of thing — its principal features or structure — in order to delimit it from other kinds and to make possible a systematic study of it and its connections.”



---

Example items:

.pull-left[
### Morphological Awareness

The book covers were heterocoloreous. What does "heterocoloreous" mean?
+ **Different Colors** (1)
+ same colors
+ bright red colors
+ different red colors
]

--

.pull-right[
### Macro and Micro Relations in Text

Glaciers are very large layers of ice that move very slowly.
Which detail is least related to this sentence?   
 + **Glaciers have been long studied by scientists.** (1)
 + Glaciers can be as large as many countries.
 + Glaciers move an average of 200 feet per year.
 + Glaciers are able to carve out rock as they move.
]
---
class: middle

### Inter-and Intra-Sentential Context Clues

Tree frogs are about two `strimes` long, which is the size of your finger. They must move `pandery` to capture flies, their favorite food. The `norkle` of tree frogs is not definitive because they move around too quickly for people to observe how long they will survive. Tree frogs must escape from `morpes`, like snakes and bats, by moving quickly. _What could `pandery` mean?_

+	**quickly** (1)
+	slowly
+	gently


---


## Key items
![Qcog3](QCOG3.jpg)

![Qcog20](QCOG20.jpg)


---

class: middle

## The IRT step:

Using similar notation to Paek & Wilson (2011)

$$P(X_{is}=1|\theta_s, g_s)=\frac{exp(\theta_s-\delta_i + \Delta G + \gamma_i*G_)}{1+ exp(\theta_s-\delta_i +\Delta G + \gamma_i*G)}$$

$\theta_s$ = ability of student `s`

$\delta_i$ = difficulty of item `i`

$\Delta$ = "Group effect" or "impact factor"

$\gamma_i$ = DIF parameter value (e.g. item*group interaction)

---

class: middle

## A Graphical Method

Let's say you've found evidence of DIF:


1. Start with a mixture model - a Latent Class Analysis, here ( we know we have different distributions)

--

2. Use multinomial logistic regression to regress class (or cluster membership) on observed group membership (for the full model)

 + Which class/cluster/profile does each group have the highest chance of being in?
 + Compare to individual cluster analyses/mixture models of each group individually
 
--

3. Compare item response profiles of clusters (do they resemble the response probabilities of observed groups given DIF)
 + An LCA with all students
 + An LCA with just heritage Spanish Speakers
 + An LCA with just non-heritage Spanish Speakers

---

## Brief Aside - Latent Class Analysis

.pull-left[

+ Highly exploratory use case here

+ Enumerate several possible potential classes (e.g. - 1 class, 2 class, 3 class)

+ Classes are categorical latent variables with reflective latent variable model interpretations

]

.pull-right[

$$P(X=(x_1..x_k)) = \Sigma_{c=1}^{G*}\pi_cP(x_1..x_k)|c)$$
+ `c` is class indicator

+ `G*` denotes total classes

+ $\pi_c$ = mixing proportions/class sizes (estimated)

+ Item responses conditionally indpendent within class

]


---
class: middle

## How did I use this process?

+ Data from a multidimensional reading test (N ~ 1000)
   + Items cherry-picked to have those that show evidence of DIF (for presentation)

+ Two primary observed groups of interest (“heritage Spanish speakers”, “non-heritage Spanish Speakers”)

+ One dimension of the test used Spanish-English cognates (in effect, a test of bilingualism)

+ Would be unfair to compare non-Spanish speakers to Spanish speakers





---
class: middle
## Question of ultimate interest:

1. Is group membership homogenous enough to perform a within group transformation of item score to equate scores across groups?

2. If so, how?

3. If not, explore potential "latent" groupings and hypothesize influence properties/sources of DIF

---

## Key items
![Qcog3](QCOG3.jpg)

![Qcog20](QCOG20.jpg)


---
class: middle

# Key point

**For this to work, I need to have a response process theory in mind**
  

- "Those who read like Spanish Speakers" and "Those who do not"

- Another way: Those who show evidence of bilingualism (those who do not)
  

---
## Settle on a full sample model and Predict into Classes (Step 1 and 2)


```{r echo=FALSE, fig.align='center', out.width="50%", fig.cap="Prediction-based: Heritage Spanish Speakers had a much higher chance of being in the Orange Class"}

knitr::include_graphics("final_mod.png")
```


---

## What about just Spanish Speakers, though?


```{r echo=FALSE, fig.align='center', out.width="50%", fig.cap="Predicted Probabilities of Just Spanish Speakers on Items – Most heritage Spanish Speakers aren’t likely to get this question correct "}

knitr::include_graphics("Just Spanish.png")


```


---
.pull-left[

```{r echo=FALSE, fig.align='left', out.width="99%", fig.cap = "Despite having a high probability (or higher probability) of being in the class most likely to get the cognate items correct - most students who identified as heritage Spanish speakers were not likely to get those items correct"}

knitr::include_graphics("Just Spanish.png")


```

]

--

.pull-right[


```{r echo=FALSE, fig.align='right', out.width="99%", fig.cap = "Response Probabilities for non-heritage Spanish Speakers"}

knitr::include_graphics("Just non-Spanish.png")
```

]

---

class: middle 

## Conclusions

+ However, this finding might tell us something about bilingualism  (moving from tests of invariance 🡪 understanding responses in terms of mixtures of populations)


+ Iterate on construct definitions - What aspect of bilingualism is/n't part of the measurand?


+ Depending on what perspective on fairness we take, how might we act in a fair way with this sort of observed grouping?


+ What about fairness?


+ How would this work for many groups or many more items? (would it scale?)


+ Next steps: Try to "explain" DIF with LLTM/explanatory IRT models? Random Item Models? DIF not as fixed quantity?


---

class: middle

## Brief Discussion of Fairness

+ For items showing evidence of DIF, the influence quantity seems to be something else behind the veil of group membership - bilingualism? Should we be using that?


+ But this uses an old conception of fairness - treat like cases alike, everybody should have equal opportunities to learn



+ What if we take a dynamic assessment perspective? E.g. - assessment mediates between student and instruction, helping student master material (a Vygotzky-esque perspective)?

---



## Thanks!

Special thanks to…


Dr. Diana Arya, UCSB Gevirtz Graduate School of Education, Associate Professor and director of the Mcenroe Reading and Language Arts Clinic for creating the Strategy Use Measure (SUM) – data from which I have interrogated against best use recommendations

As well as other committee members:

Dr. Andy Maul

Dr. Karen Nylund-Gibson

---
## References

Arya, D., Clairmont, A., Katz, D., & Maul, A. (2020). Measuring Reading Strategy Use. _Educational Assessment_, 25(1), 5-30.

Borsboom, D., Mellenbergh, G. J., & Van Heerden, J. (2002). Different Kinds of DIF: A Distinction Between Absolute and Relative Forms of Measurement Invariance and Bias. _Applied Psychological Measurement_, 26(4), 433–450. [https://doi.org/10.1177/014662102237798](https://doi.org/10.1177/014662102237798)  

Paek, I., & Wilson, M. (2011). Formulating the Rasch Differential Item Functioning Model Under the Marginal Maximum Likelihood Estimation Context and Its Comparison With Mantel–Haenszel Procedure in Short Test and Small Sample Conditions. _Educational and Psychological Measurement_, 71(6), 1023–1046. [https://doi.org/10.1177/0013164411400734](https://doi.org/10.1177/0013164411400734)




---




